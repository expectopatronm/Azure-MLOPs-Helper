{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from helper import VMManager\n",
    "\n",
    "my_vm_manager = VMManager()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Falcon Deployment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_vm = my_vm_manager.start_vm('llm-inference-vm-2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ssh = my_vm_manager.establish_ssh_connection_to_vm(public_ip='20.23.216.140', port='222', username='llm-inference-vm-2-user', key_path='keys/llm-inference-vm-2_key.pem')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use this only if necessary.\n",
    "command = \"\"\"sudo docker stop $(sudo docker ps -aq)\"\"\"\n",
    "my_vm_manager.run_ssh_command_on_vm(ssh=ssh, command=command)\n",
    "command = \"\"\"sudo docker rm $(sudo docker ps -aq)\"\"\"\n",
    "my_vm_manager.run_ssh_command_on_vm(ssh=ssh, command=command)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "command = \"\"\"sudo docker run -d --name falcon-40b --gpus all --shm-size 1g -p 8080:80 -v $PWD/data:/data ghcr.io/huggingface/text-generation-inference:0.9 --model-id 'tiiuae/falcon-40b-instruct' --num-shard 2 --trust-remote-code --revision 1e7fdcc9f45d13704f3826e99937917e007cd975\"\"\"\n",
    "my_vm_manager.run_ssh_command_on_vm(ssh=ssh, command=command)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "command = \"\"\"sudo docker ps\"\"\"\n",
    "my_vm_manager.run_ssh_command_on_vm(ssh=ssh, command=command)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Should be enough to simply start the container\n",
    "command = \"\"\"sudo docker start falcon-40b\"\"\"\n",
    "my_vm_manager.run_ssh_command_on_vm(ssh=ssh, command=command)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Looking at logs\n",
    "command = \"\"\"sudo docker logs falcon-40b\"\"\"\n",
    "my_vm_manager.run_ssh_command_on_vm(ssh=ssh, command=command)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from text_generation import Client\n",
    "\n",
    "client = Client(\"http://20.23.216.140:8082\", timeout=30)\n",
    "print(client.generate(\"Tell me about Einstein.\", max_new_tokens=256).generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LLama Deployment 13B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_vm = my_vm_manager.start_vm('llm-inference-vm-1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ssh = my_vm_manager.establish_ssh_connection_to_vm(public_ip='20.23.216.140', port='221', username='llm-inference-vm-1-user', key_path='keys/llm-inference-vm-1_key.pem')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use this only if necessary.\n",
    "command = \"\"\"sudo docker stop $(sudo docker ps -aq)\"\"\"\n",
    "my_vm_manager.run_ssh_command_on_vm(ssh=ssh, command=command)\n",
    "command = \"\"\"sudo docker rm $(sudo docker ps -aq)\"\"\"\n",
    "my_vm_manager.run_ssh_command_on_vm(ssh=ssh, command=command)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "command = \"\"\"sudo docker run -d --name llama-13b --gpus all --shm-size 1g -p 8080:80 -e HUGGING_FACE_HUB_TOKEN=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 -v $PWD/data:/data ghcr.io/huggingface/text-generation-inference:0.9 --model-id 'meta-llama/Llama-2-13b-chat-hf' --num-shard 1 --trust-remote-code\"\"\"\n",
    "my_vm_manager.run_ssh_command_on_vm(ssh=ssh, command=command)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "command = \"\"\"sudo docker ps\"\"\"\n",
    "my_vm_manager.run_ssh_command_on_vm(ssh=ssh, command=command)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Should be enough to simply start the container\n",
    "command = \"\"\"sudo docker start llama-13b\"\"\"\n",
    "my_vm_manager.run_ssh_command_on_vm(ssh=ssh, command=command)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Looking at logs\n",
    "command = \"\"\"sudo docker logs llama-13b\"\"\"\n",
    "my_vm_manager.run_ssh_command_on_vm(ssh=ssh, command=command)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from text_generation import Client\n",
    "\n",
    "client = Client(\"http://20.23.216.140:8081\", timeout=30)\n",
    "print(client.generate(\"Tell me about Einstein.\", max_new_tokens=256).generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LLama Deployment 70B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_vm = my_vm_manager.start_vm('llm-inference-vm-3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ssh = my_vm_manager.establish_ssh_connection_to_vm(public_ip='20.23.216.140', port='223', username='llm-inference-vm-3-user', key_path='keys/llm-inference-vm-3_key.pem')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use this only if necessary.\n",
    "command = \"\"\"sudo docker stop $(sudo docker ps -aq)\"\"\"\n",
    "my_vm_manager.run_ssh_command_on_vm(ssh=ssh, command=command)\n",
    "command = \"\"\"sudo docker rm $(sudo docker ps -aq)\"\"\"\n",
    "my_vm_manager.run_ssh_command_on_vm(ssh=ssh, command=command)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use this only if necessary.\n",
    "command = \"\"\"sudo docker run -d --name llama-70b --gpus all --shm-size 1g -p 8080:80 -e HUGGING_FACE_HUB_TOKEN=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 -v $PWD/data:/data ghcr.io/huggingface/text-generation-inference:latest --model-id 'meta-llama/Llama-2-70b-chat-hf' --num-shard 4 --max-input-length 4096 --max-total-tokens 5120 --max-batch-prefill-tokens 10240 --trust-remote-code\"\"\"\n",
    "my_vm_manager.run_ssh_command_on_vm(ssh=ssh, command=command)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start the container\n",
    "command = \"\"\"sudo docker start llama-70b\"\"\"\n",
    "my_vm_manager.run_ssh_command_on_vm(ssh=ssh, command=command)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Looking at logs\n",
    "command = \"\"\"sudo docker logs llama-70b\"\"\"\n",
    "my_vm_manager.run_ssh_command_on_vm(ssh=ssh, command=command)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from text_generation import Client\n",
    "\n",
    "client = Client(\"http://20.23.216.140:8083\", timeout=30)\n",
    "print(client.generate(\"Tell me about Einstein.\", max_new_tokens=256).generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# H2O GPT 4096 LLama Deployment 70B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_vm = my_vm_manager.start_vm('llm-inference-vm-4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ssh = my_vm_manager.establish_ssh_connection_to_vm(public_ip='20.23.216.140', port='224', username='llm-inference-vm-4-user', key_path='keys/llm-inference-vm-4_key.pem')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use this only if necessary.\n",
    "command = \"\"\"sudo docker stop $(sudo docker ps -aq)\"\"\"\n",
    "my_vm_manager.run_ssh_command_on_vm(ssh=ssh, command=command)\n",
    "command = \"\"\"sudo docker rm $(sudo docker ps -aq)\"\"\"\n",
    "my_vm_manager.run_ssh_command_on_vm(ssh=ssh, command=command)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "command = \"\"\"sudo docker run -d --name h2ogpt-4096-llama2-70b-chat --gpus all --shm-size 1g -p 8080:80 -e HUGGING_FACE_HUB_TOKEN=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 -v $PWD/data:/data ghcr.io/huggingface/text-generation-inference:latest --model-id 'h2oai/h2ogpt-4096-llama2-70b-chat' --num-shard 4 --max-input-length 4096 --max-total-tokens 5120 --max-batch-prefill-tokens 10240 --trust-remote-code\"\"\"\n",
    "my_vm_manager.run_ssh_command_on_vm(ssh=ssh, command=command)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "command = \"\"\"sudo docker ps\"\"\"\n",
    "my_vm_manager.run_ssh_command_on_vm(ssh=ssh, command=command)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Should be enough to simply start the container\n",
    "command = \"\"\"sudo docker start h2ogpt-4096-llama2-70b-chat\"\"\"\n",
    "my_vm_manager.run_ssh_command_on_vm(ssh=ssh, command=command)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Looking at logs\n",
    "command = \"\"\"sudo docker logs h2ogpt-4096-llama2-70b-chat\"\"\"\n",
    "my_vm_manager.run_ssh_command_on_vm(ssh=ssh, command=command)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from text_generation import Client\n",
    "\n",
    "client = Client(\"http://20.23.216.140:8084\", timeout=30)\n",
    "print(client.generate(\"Tell me about Einstein.\", max_new_tokens=256).generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# QA Car Manual Bot VM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_vm = my_vm_manager.start_vm('qa-car-manual-chatbot-vm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SSH into the VM using the following command via a terminal\n",
    "# ssh -p 226 -i qa-car-manual-chatbot-vm_key.pem qa-car-manual-chatbot-vm-user@20.23.216.140\n",
    "\n",
    "# and execute the following commands\n",
    "# cd qa_car_manual\n",
    "# sh car-manual-bot-startscript.sh"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
